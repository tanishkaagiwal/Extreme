{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Encoding & Merging complete! Merged dataset shape: (36000, 31)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MultiLabelBinarizer\n",
    "\n",
    "# Load datasets\n",
    "static_df = pd.read_csv(\"/Users/isha/Datathon/venv/static_client_data.csv\")\n",
    "time_series_df = pd.read_csv(\"/Users/isha/Datathon/adjusted_timeseries_SCN_02.csv\")\n",
    "\n",
    "\n",
    "# Convert preferred_asset_classes to lists (if stored as strings)\n",
    "static_df[\"preferred_asset_classes\"] = static_df[\"preferred_asset_classes\"].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# One-Hot Encode categorical columns\n",
    "one_hot_cols = [\"gender\", \"employment_status\", \"investment_goals\"]\n",
    "one_hot_encoder = OneHotEncoder(drop=\"first\", sparse_output=False)\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(static_df[one_hot_cols])\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=one_hot_encoder.get_feature_names_out(one_hot_cols))\n",
    "\n",
    "# Label Encode risk_appetite\n",
    "label_encoder = LabelEncoder()\n",
    "static_df[\"risk_appetite_encoded\"] = label_encoder.fit_transform(static_df[\"risk_appetite\"])\n",
    "\n",
    "# Multi-Hot Encode preferred_asset_classes\n",
    "mlb = MultiLabelBinarizer()\n",
    "multi_hot_encoded = mlb.fit_transform(static_df[\"preferred_asset_classes\"])\n",
    "multi_hot_df = pd.DataFrame(multi_hot_encoded, columns=[f\"asset_{cls}\" for cls in mlb.classes_])\n",
    "\n",
    "# Drop original categorical columns and concatenate encoded features\n",
    "static_df.drop(columns=one_hot_cols + [\"risk_appetite\", \"preferred_asset_classes\"], inplace=True)\n",
    "static_df = pd.concat([static_df, one_hot_df, multi_hot_df], axis=1)\n",
    "\n",
    "# Merge with time-series data (preserving all time-series rows)\n",
    "merged_df = time_series_df.merge(static_df, on=\"client_id\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save processed dataset\n",
    "merged_df.to_csv(\"/Users/isha/Datathon/static_time_series_ps3.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Encoding & Merging complete! Merged dataset shape:\", merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36000, 9)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_df.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check stationarity for each client and feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 client_id                      feature  \\\n",
      "0     2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2              portfolio_value   \n",
      "1     2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2        equity_allocation_pct   \n",
      "2     2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2  fixed_income_allocation_pct   \n",
      "3     2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2         monthly_contribution   \n",
      "4     2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2      market_volatility_index   \n",
      "...                                    ...                          ...   \n",
      "6995  41b1bc9d-b7ec-4092-803b-6ae2a1023605  fixed_income_allocation_pct   \n",
      "6996  41b1bc9d-b7ec-4092-803b-6ae2a1023605         monthly_contribution   \n",
      "6997  41b1bc9d-b7ec-4092-803b-6ae2a1023605      market_volatility_index   \n",
      "6998  41b1bc9d-b7ec-4092-803b-6ae2a1023605          macroeconomic_score   \n",
      "6999  41b1bc9d-b7ec-4092-803b-6ae2a1023605              sentiment_index   \n",
      "\n",
      "      adf_statistic       p_value  is_stationary  \n",
      "0         -1.729305  4.160802e-01          False  \n",
      "1         -6.351870  2.598958e-08           True  \n",
      "2         -6.351870  2.598958e-08           True  \n",
      "3         -5.767385  5.492363e-07           True  \n",
      "4         -5.626522  1.114155e-06           True  \n",
      "...             ...           ...            ...  \n",
      "6995      -3.633051  5.159703e-03           True  \n",
      "6996      -5.908753  2.669727e-07           True  \n",
      "6997      -2.912684  4.389274e-02           True  \n",
      "6998      -5.612939  1.192046e-06           True  \n",
      "6999      -4.057587  1.137242e-03           True  \n",
      "\n",
      "[7000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "df = pd.read_csv(\"/Users/isha/Datathon/static_time_series_ps3.csv\")\n",
    "\n",
    "time_features = [\n",
    "    'portfolio_value', \n",
    "    'equity_allocation_pct', \n",
    "    'fixed_income_allocation_pct', \n",
    "    'monthly_contribution', \n",
    "    'market_volatility_index', \n",
    "    'macroeconomic_score', \n",
    "    'sentiment_index'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "all_results = []\n",
    "\n",
    "# Check stationarity for each client and feature\n",
    "for client_id in df['client_id'].unique():\n",
    "    client_data = df[df['client_id'] == client_id]\n",
    "    \n",
    "    for feature in time_features:\n",
    "        # Extract the time series for the current feature\n",
    "        series = client_data[feature].values\n",
    "        \n",
    "        # Run ADF test\n",
    "        result = adfuller(series, autolag='AIC')\n",
    "        \n",
    "        # Store results in the list\n",
    "        all_results.append({\n",
    "            'client_id': client_id,\n",
    "            'feature': feature,\n",
    "            'adf_statistic': result[0],\n",
    "            'p_value': result[1],\n",
    "            'is_stationary': result[1] < 0.05  # p-value < 0.05 â†’ stationarity\n",
    "        })\n",
    "\n",
    "# Create the DataFrame after the loop\n",
    "stationarity_results = pd.DataFrame(all_results)\n",
    "\n",
    "# Save or display results\n",
    "print(stationarity_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply differencing if non-stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Stationarity transformation applied to all features and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"/Users/isha/Datathon/static_time_series_ps3.csv\")\n",
    "\n",
    "# List of time-series features to check\n",
    "time_features = [\n",
    "    'portfolio_value', \n",
    "    'equity_allocation_pct', \n",
    "    'fixed_income_allocation_pct', \n",
    "    'monthly_contribution', \n",
    "    'market_volatility_index', \n",
    "    'macroeconomic_score', \n",
    "    'sentiment_index'\n",
    "]\n",
    "\n",
    "# List to store transformed data\n",
    "differenced_data = []\n",
    "\n",
    "# Loop over each client\n",
    "for client_id in df['client_id'].unique():\n",
    "    client_data = df[df['client_id'] == client_id].copy()\n",
    "\n",
    "    for feature in time_features:\n",
    "        series = client_data[feature].values\n",
    "\n",
    "        # ADF test\n",
    "        try:\n",
    "            p_value = adfuller(series, autolag='AIC')[1]\n",
    "        except:\n",
    "            p_value = 1  # Treat as non-stationary if test fails\n",
    "\n",
    "        # Apply differencing if non-stationary\n",
    "        if p_value >= 0.05:\n",
    "            diff_series = np.diff(series)\n",
    "            diff_series = np.insert(diff_series, 0, 0)  # Pad to maintain length\n",
    "        else:\n",
    "            diff_series = series\n",
    "\n",
    "        # Add new stationary feature column\n",
    "        client_data[feature + '_stationary'] = diff_series\n",
    "\n",
    "    # Store updated client data\n",
    "    differenced_data.append(client_data)\n",
    "\n",
    "# Combine everything back\n",
    "df_stationary = pd.concat(differenced_data, ignore_index=True)\n",
    "\n",
    "# Save to file\n",
    "df_stationary.to_csv(\"/Users/isha/Datathon/stationary_data1.csv\", index=False)\n",
    "print(\"âœ… Stationarity transformation applied to all features and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>month</th>\n",
       "      <th>portfolio_value</th>\n",
       "      <th>equity_allocation_pct</th>\n",
       "      <th>fixed_income_allocation_pct</th>\n",
       "      <th>monthly_contribution</th>\n",
       "      <th>market_volatility_index</th>\n",
       "      <th>macroeconomic_score</th>\n",
       "      <th>sentiment_index</th>\n",
       "      <th>age</th>\n",
       "      <th>...</th>\n",
       "      <th>asset_Mutual Funds</th>\n",
       "      <th>asset_Real Estate</th>\n",
       "      <th>asset_Stocks</th>\n",
       "      <th>portfolio_value_stationary</th>\n",
       "      <th>equity_allocation_pct_stationary</th>\n",
       "      <th>fixed_income_allocation_pct_stationary</th>\n",
       "      <th>monthly_contribution_stationary</th>\n",
       "      <th>market_volatility_index_stationary</th>\n",
       "      <th>macroeconomic_score_stationary</th>\n",
       "      <th>sentiment_index_stationary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2</td>\n",
       "      <td>01-03-2022</td>\n",
       "      <td>53094.36</td>\n",
       "      <td>63.89460</td>\n",
       "      <td>32.72620</td>\n",
       "      <td>1188.90</td>\n",
       "      <td>17.16</td>\n",
       "      <td>3.83</td>\n",
       "      <td>7.25</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>63.89460</td>\n",
       "      <td>32.72620</td>\n",
       "      <td>1188.90</td>\n",
       "      <td>17.16</td>\n",
       "      <td>3.83</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2</td>\n",
       "      <td>01-04-2022</td>\n",
       "      <td>54580.94</td>\n",
       "      <td>57.82784</td>\n",
       "      <td>39.74048</td>\n",
       "      <td>1738.97</td>\n",
       "      <td>12.01</td>\n",
       "      <td>2.56</td>\n",
       "      <td>3.82</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1486.58</td>\n",
       "      <td>57.82784</td>\n",
       "      <td>39.74048</td>\n",
       "      <td>1738.97</td>\n",
       "      <td>12.01</td>\n",
       "      <td>2.56</td>\n",
       "      <td>3.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2</td>\n",
       "      <td>01-05-2022</td>\n",
       "      <td>56255.62</td>\n",
       "      <td>37.69136</td>\n",
       "      <td>63.02192</td>\n",
       "      <td>1175.53</td>\n",
       "      <td>24.98</td>\n",
       "      <td>4.62</td>\n",
       "      <td>6.21</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1674.68</td>\n",
       "      <td>37.69136</td>\n",
       "      <td>63.02192</td>\n",
       "      <td>1175.53</td>\n",
       "      <td>24.98</td>\n",
       "      <td>4.62</td>\n",
       "      <td>6.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2</td>\n",
       "      <td>01-06-2022</td>\n",
       "      <td>58008.47</td>\n",
       "      <td>44.51416</td>\n",
       "      <td>55.13352</td>\n",
       "      <td>1968.66</td>\n",
       "      <td>22.68</td>\n",
       "      <td>1.99</td>\n",
       "      <td>6.49</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1752.85</td>\n",
       "      <td>44.51416</td>\n",
       "      <td>55.13352</td>\n",
       "      <td>1968.66</td>\n",
       "      <td>22.68</td>\n",
       "      <td>1.99</td>\n",
       "      <td>6.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2</td>\n",
       "      <td>01-07-2022</td>\n",
       "      <td>59831.30</td>\n",
       "      <td>36.42822</td>\n",
       "      <td>64.48234</td>\n",
       "      <td>1529.49</td>\n",
       "      <td>11.39</td>\n",
       "      <td>2.23</td>\n",
       "      <td>7.39</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1822.83</td>\n",
       "      <td>36.42822</td>\n",
       "      <td>64.48234</td>\n",
       "      <td>1529.49</td>\n",
       "      <td>11.39</td>\n",
       "      <td>2.23</td>\n",
       "      <td>7.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35995</th>\n",
       "      <td>41b1bc9d-b7ec-4092-803b-6ae2a1023605</td>\n",
       "      <td>01-10-2024</td>\n",
       "      <td>56962.78</td>\n",
       "      <td>19.08540</td>\n",
       "      <td>84.53380</td>\n",
       "      <td>1862.76</td>\n",
       "      <td>20.85</td>\n",
       "      <td>1.45</td>\n",
       "      <td>5.99</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1884.81</td>\n",
       "      <td>19.08540</td>\n",
       "      <td>84.53380</td>\n",
       "      <td>1862.76</td>\n",
       "      <td>20.85</td>\n",
       "      <td>1.45</td>\n",
       "      <td>5.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35996</th>\n",
       "      <td>41b1bc9d-b7ec-4092-803b-6ae2a1023605</td>\n",
       "      <td>01-11-2024</td>\n",
       "      <td>59890.52</td>\n",
       "      <td>43.59216</td>\n",
       "      <td>56.19952</td>\n",
       "      <td>1926.25</td>\n",
       "      <td>22.79</td>\n",
       "      <td>3.21</td>\n",
       "      <td>5.02</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2927.74</td>\n",
       "      <td>43.59216</td>\n",
       "      <td>56.19952</td>\n",
       "      <td>1926.25</td>\n",
       "      <td>22.79</td>\n",
       "      <td>3.21</td>\n",
       "      <td>5.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35997</th>\n",
       "      <td>41b1bc9d-b7ec-4092-803b-6ae2a1023605</td>\n",
       "      <td>01-12-2024</td>\n",
       "      <td>60708.53</td>\n",
       "      <td>28.86782</td>\n",
       "      <td>73.22354</td>\n",
       "      <td>1958.70</td>\n",
       "      <td>15.64</td>\n",
       "      <td>3.41</td>\n",
       "      <td>3.82</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>818.01</td>\n",
       "      <td>28.86782</td>\n",
       "      <td>73.22354</td>\n",
       "      <td>1958.70</td>\n",
       "      <td>15.64</td>\n",
       "      <td>3.41</td>\n",
       "      <td>3.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35998</th>\n",
       "      <td>41b1bc9d-b7ec-4092-803b-6ae2a1023605</td>\n",
       "      <td>01-01-2025</td>\n",
       "      <td>61984.00</td>\n",
       "      <td>40.88148</td>\n",
       "      <td>59.33356</td>\n",
       "      <td>1284.70</td>\n",
       "      <td>26.19</td>\n",
       "      <td>2.17</td>\n",
       "      <td>5.82</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1275.47</td>\n",
       "      <td>40.88148</td>\n",
       "      <td>59.33356</td>\n",
       "      <td>1284.70</td>\n",
       "      <td>26.19</td>\n",
       "      <td>2.17</td>\n",
       "      <td>5.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35999</th>\n",
       "      <td>41b1bc9d-b7ec-4092-803b-6ae2a1023605</td>\n",
       "      <td>01-02-2025</td>\n",
       "      <td>62743.82</td>\n",
       "      <td>50.64546</td>\n",
       "      <td>48.04462</td>\n",
       "      <td>631.58</td>\n",
       "      <td>25.81</td>\n",
       "      <td>2.87</td>\n",
       "      <td>5.52</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>759.82</td>\n",
       "      <td>50.64546</td>\n",
       "      <td>48.04462</td>\n",
       "      <td>631.58</td>\n",
       "      <td>25.81</td>\n",
       "      <td>2.87</td>\n",
       "      <td>5.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36000 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  client_id       month  portfolio_value  \\\n",
       "0      2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2  01-03-2022         53094.36   \n",
       "1      2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2  01-04-2022         54580.94   \n",
       "2      2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2  01-05-2022         56255.62   \n",
       "3      2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2  01-06-2022         58008.47   \n",
       "4      2eef3ba0-a7c7-42b1-a3ac-3c33aedcb5f2  01-07-2022         59831.30   \n",
       "...                                     ...         ...              ...   \n",
       "35995  41b1bc9d-b7ec-4092-803b-6ae2a1023605  01-10-2024         56962.78   \n",
       "35996  41b1bc9d-b7ec-4092-803b-6ae2a1023605  01-11-2024         59890.52   \n",
       "35997  41b1bc9d-b7ec-4092-803b-6ae2a1023605  01-12-2024         60708.53   \n",
       "35998  41b1bc9d-b7ec-4092-803b-6ae2a1023605  01-01-2025         61984.00   \n",
       "35999  41b1bc9d-b7ec-4092-803b-6ae2a1023605  01-02-2025         62743.82   \n",
       "\n",
       "       equity_allocation_pct  fixed_income_allocation_pct  \\\n",
       "0                   63.89460                     32.72620   \n",
       "1                   57.82784                     39.74048   \n",
       "2                   37.69136                     63.02192   \n",
       "3                   44.51416                     55.13352   \n",
       "4                   36.42822                     64.48234   \n",
       "...                      ...                          ...   \n",
       "35995               19.08540                     84.53380   \n",
       "35996               43.59216                     56.19952   \n",
       "35997               28.86782                     73.22354   \n",
       "35998               40.88148                     59.33356   \n",
       "35999               50.64546                     48.04462   \n",
       "\n",
       "       monthly_contribution  market_volatility_index  macroeconomic_score  \\\n",
       "0                   1188.90                    17.16                 3.83   \n",
       "1                   1738.97                    12.01                 2.56   \n",
       "2                   1175.53                    24.98                 4.62   \n",
       "3                   1968.66                    22.68                 1.99   \n",
       "4                   1529.49                    11.39                 2.23   \n",
       "...                     ...                      ...                  ...   \n",
       "35995               1862.76                    20.85                 1.45   \n",
       "35996               1926.25                    22.79                 3.21   \n",
       "35997               1958.70                    15.64                 3.41   \n",
       "35998               1284.70                    26.19                 2.17   \n",
       "35999                631.58                    25.81                 2.87   \n",
       "\n",
       "       sentiment_index  age  ...  asset_Mutual Funds  asset_Real Estate  \\\n",
       "0                 7.25   53  ...                   0                  1   \n",
       "1                 3.82   53  ...                   0                  1   \n",
       "2                 6.21   53  ...                   0                  1   \n",
       "3                 6.49   53  ...                   0                  1   \n",
       "4                 7.39   53  ...                   0                  1   \n",
       "...                ...  ...  ...                 ...                ...   \n",
       "35995             5.99   64  ...                   0                  1   \n",
       "35996             5.02   64  ...                   0                  1   \n",
       "35997             3.82   64  ...                   0                  1   \n",
       "35998             5.82   64  ...                   0                  1   \n",
       "35999             5.52   64  ...                   0                  1   \n",
       "\n",
       "       asset_Stocks  portfolio_value_stationary  \\\n",
       "0                 0                        0.00   \n",
       "1                 0                     1486.58   \n",
       "2                 0                     1674.68   \n",
       "3                 0                     1752.85   \n",
       "4                 0                     1822.83   \n",
       "...             ...                         ...   \n",
       "35995             0                     1884.81   \n",
       "35996             0                     2927.74   \n",
       "35997             0                      818.01   \n",
       "35998             0                     1275.47   \n",
       "35999             0                      759.82   \n",
       "\n",
       "       equity_allocation_pct_stationary  \\\n",
       "0                              63.89460   \n",
       "1                              57.82784   \n",
       "2                              37.69136   \n",
       "3                              44.51416   \n",
       "4                              36.42822   \n",
       "...                                 ...   \n",
       "35995                          19.08540   \n",
       "35996                          43.59216   \n",
       "35997                          28.86782   \n",
       "35998                          40.88148   \n",
       "35999                          50.64546   \n",
       "\n",
       "       fixed_income_allocation_pct_stationary  \\\n",
       "0                                    32.72620   \n",
       "1                                    39.74048   \n",
       "2                                    63.02192   \n",
       "3                                    55.13352   \n",
       "4                                    64.48234   \n",
       "...                                       ...   \n",
       "35995                                84.53380   \n",
       "35996                                56.19952   \n",
       "35997                                73.22354   \n",
       "35998                                59.33356   \n",
       "35999                                48.04462   \n",
       "\n",
       "       monthly_contribution_stationary  market_volatility_index_stationary  \\\n",
       "0                              1188.90                               17.16   \n",
       "1                              1738.97                               12.01   \n",
       "2                              1175.53                               24.98   \n",
       "3                              1968.66                               22.68   \n",
       "4                              1529.49                               11.39   \n",
       "...                                ...                                 ...   \n",
       "35995                          1862.76                               20.85   \n",
       "35996                          1926.25                               22.79   \n",
       "35997                          1958.70                               15.64   \n",
       "35998                          1284.70                               26.19   \n",
       "35999                           631.58                               25.81   \n",
       "\n",
       "       macroeconomic_score_stationary  sentiment_index_stationary  \n",
       "0                                3.83                        7.25  \n",
       "1                                2.56                        3.82  \n",
       "2                                4.62                        6.21  \n",
       "3                                1.99                        6.49  \n",
       "4                                2.23                        7.39  \n",
       "...                               ...                         ...  \n",
       "35995                            1.45                        5.99  \n",
       "35996                            3.21                        5.02  \n",
       "35997                            3.41                        3.82  \n",
       "35998                            2.17                        5.82  \n",
       "35999                            2.87                        5.52  \n",
       "\n",
       "[36000 rows x 38 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stationary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering: lag and rolling features added to stationary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Feature engineering complete: lag and rolling features added to stationary columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the stationary dataset\n",
    "df = pd.read_csv(\"/Users/isha/Datathon/stationary_data1.csv\")\n",
    "\n",
    "# âœ… Only apply to stationary-transformed columns\n",
    "features_to_engineer = [\n",
    "    'portfolio_value_stationary',\n",
    "    'monthly_contribution_stationary',\n",
    "    'market_volatility_index_stationary',\n",
    "    'sentiment_index_stationary',\n",
    "    'macroeconomic_score_stationary'\n",
    "]\n",
    "\n",
    "# ---------- Function: Add Lag Features ----------\n",
    "def add_lag_features(df, features, lags=[1, 3, 6]):\n",
    "    df_lagged = df.copy()\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            df_lagged[f\"{feature}_lag_{lag}\"] = df_lagged.groupby(\"client_id\")[feature].shift(lag)\n",
    "    return df_lagged\n",
    "\n",
    "# ---------- Function: Add Rolling Statistics ----------\n",
    "def add_rolling_features(df, features, windows=[3, 6]):\n",
    "    df_rolled = df.copy()\n",
    "    for feature in features:\n",
    "        for window in windows:\n",
    "            df_rolled[f\"{feature}_roll_mean_{window}\"] = (\n",
    "                df_rolled.groupby(\"client_id\")[feature].transform(lambda x: x.rolling(window).mean())\n",
    "            )\n",
    "            df_rolled[f\"{feature}_roll_std_{window}\"] = (\n",
    "                df_rolled.groupby(\"client_id\")[feature].transform(lambda x: x.rolling(window).std())\n",
    "            )\n",
    "    return df_rolled\n",
    "\n",
    "# ---------- Apply Lag + Rolling Feature Engineering ----------\n",
    "df = add_lag_features(df, features=features_to_engineer, lags=[1, 3, 6])\n",
    "df = add_rolling_features(df, features=features_to_engineer, windows=[3, 6])\n",
    "\n",
    "# ---------- Drop rows with NaNs introduced by shifting/rolling ----------\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# ---------- Save the engineered dataset ----------\n",
    "df.to_csv(\"/Users/isha/Datathon/engineered_stationary_data1.csv\", index=False)\n",
    "print(\"âœ… Feature engineering complete: lag and rolling features added to stationary columns.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engineer new features: portfolio_growth_rate, contribution_ratio, eq_fi_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "âœ… Predictions saved to: /Users/isha/Datathon/predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# --- Config ---\n",
    "test_file = \"/Users/isha/Datathon/stationary_data_with_smart_features1.csv\"\n",
    "model_path = \"/Users/isha/Datathon/lstm_portfolio_forecast_model.keras\"  # Update if different\n",
    "output_file = \"/Users/isha/Datathon/predictions.csv\"\n",
    "sequence_length = 24\n",
    "\n",
    "\n",
    "\n",
    "# --- Features used in training ---\n",
    "selected_features = ['employment_status_Unemployed', 'investment_goals_Wealth Accumulation', 'asset_Real Estate',\n",
    "                     'risk_appetite_encoded', 'market_volatility_index_stationary_roll_mean_6',\n",
    "                     'investment_goals_Home Purchase', 'eq_fi_ratio', 'savings_rate',\n",
    "                     'sentiment_index_stationary_roll_mean_6', 'age', 'employment_status_Self-Employed',\n",
    "                     'annual_income', 'asset_Bonds', 'macroeconomic_score_stationary_roll_mean_3',\n",
    "                     'debt_to_income_ratio', 'financial_knowledge_score', 'investment_goals_Retirement',\n",
    "                     'asset_Mutual Funds', 'asset_Stocks', 'net_worth', 'monthly_contribution_stationary_roll_mean_6',\n",
    "                     'gender_Male', 'gender_Other', 'monthly_contribution_stationary_roll_mean_3',\n",
    "                     'dependents', 'investment_horizon_years', 'employment_status_Salaried',\n",
    "                     'sentiment_index_stationary_roll_mean_3', 'market_volatility_index_stationary_roll_mean_3',\n",
    "                     'asset_ETFs', 'macroeconomic_score_stationary_roll_mean_6']\n",
    "\n",
    "# --- Load test data ---\n",
    "df = pd.read_csv(test_file)\n",
    "\n",
    "# Drop rows with missing values in selected features\n",
    "df = df.dropna(subset=selected_features)\n",
    "\n",
    "# Sort by client_id and month\n",
    "df = df.sort_values(['client_id', 'month'])\n",
    "\n",
    "# Normalize features\n",
    "feature_scaler = MinMaxScaler()\n",
    "df[selected_features] = feature_scaler.fit_transform(df[selected_features])\n",
    "\n",
    "# Dummy scaler for inverse transform later (we just need shape and min/max)\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaler.fit(np.zeros((1, 3)))  # Just for inverse_transform to work\n",
    "\n",
    "# --- Sequence Creation ---\n",
    "def create_sequences(data, features, seq_len):\n",
    "    X, client_ids = [], []\n",
    "    grouped = data.groupby('client_id')\n",
    "    for client_id, group in grouped:\n",
    "        group = group.reset_index(drop=True)\n",
    "        if len(group) >= seq_len:\n",
    "            for i in range(len(group) - seq_len + 1):\n",
    "                seq_x = group.loc[i:i+seq_len-1, features].values.astype(np.float32)\n",
    "                X.append(seq_x)\n",
    "                client_ids.append(client_id)\n",
    "    return np.array(X), client_ids\n",
    "\n",
    "X_test, client_ids = create_sequences(df, selected_features, sequence_length)\n",
    "\n",
    "# --- Load model ---\n",
    "model = load_model(model_path, compile=False)\n",
    "\n",
    "\n",
    "# --- Predict ---\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# --- Save predictions ---\n",
    "pred_df = pd.DataFrame(y_pred, columns=['forecasted_value_year_1', 'forecasted_value_year_2', 'forecasted_value_year_3'])\n",
    "pred_df['client_id'] = client_ids\n",
    "pred_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"âœ… Predictions saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Unscaled predictions saved to: /Users/isha/Datathon/predictions.csv\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "\n",
    "target_scaler = load('/Users/isha/Datathon/target_scaler.save')  # adjust path if needed\n",
    "\n",
    "\n",
    "# --- Inverse transform ---\n",
    "y_pred_unscaled = target_scaler.inverse_transform(y_pred)\n",
    "\n",
    "# --- Save unscaled predictions ---\n",
    "unscaled_pred_df = pd.DataFrame(y_pred_unscaled, columns=['forecasted_value_year_1', 'forecasted_value_year_2', 'forecasted_value_year_3'])\n",
    "unscaled_pred_df['client_id'] = client_ids\n",
    "unscaled_pred_df.to_csv(\"/Users/isha/Datathon/predictions.csv\", index=False)\n",
    "\n",
    "print(\"ğŸ“ˆ Unscaled predictions saved to: /Users/isha/Datathon/predictions.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in ./venv/lib/python3.10/site-packages (1.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added 3 smart financial features and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'df' is your original or stationary DataFrame\n",
    "df = pd.read_csv(\"/Users/isha/Datathon/engineered_stationary_data1.csv\")\n",
    "\n",
    "# === Feature 1: Portfolio Growth Rate ===\n",
    "df['portfolio_growth_rate'] = df.groupby('client_id')['portfolio_value_stationary'].pct_change()\n",
    "\n",
    "# === Feature 2: Contribution Ratio ===\n",
    "df['contribution_ratio'] = df['monthly_contribution_stationary'] / (df['portfolio_value_stationary'] + 1e-6)\n",
    "\n",
    "# === Feature 3: Equity to Fixed Income Ratio ===\n",
    "df['eq_fi_ratio'] = df['equity_allocation_pct_stationary'] / (df['fixed_income_allocation_pct_stationary'] + 1e-6)\n",
    "\n",
    "# Optional: Fill NaNs from pct_change or divide-by-zero issues\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Save updated DataFrame\n",
    "df.to_csv(\"/Users/isha/Datathon/stationary_data_with_smart_features1.csv\", index=False)\n",
    "print(\"âœ… Added 3 smart financial features and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (7000, 24, 31)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"/Users/isha/Datathon/engineered_stationary_data.csv\")\n",
    "\n",
    "# Define features and targets\n",
    "selected_features = [\n",
    "    'employment_status_Unemployed', 'investment_goals_Wealth Accumulation', 'asset_Real Estate',\n",
    "    'risk_appetite_encoded', 'market_volatility_index_stationary_roll_mean_6', 'investment_goals_Home Purchase',\n",
    "    'savings_rate', 'sentiment_index_stationary_roll_mean_6', 'age', 'employment_status_Self-Employed',\n",
    "    'annual_income', 'asset_Bonds', 'macroeconomic_score_stationary_roll_mean_3', 'debt_to_income_ratio',\n",
    "    'financial_knowledge_score', 'investment_goals_Retirement', 'fixed_income_allocation_pct_stationary',\n",
    "    'asset_Mutual Funds', 'asset_Stocks', 'net_worth', 'monthly_contribution_stationary_roll_mean_6', 'gender_Male',\n",
    "    'equity_allocation_pct_stationary', 'gender_Other', 'monthly_contribution_stationary_roll_mean_3', 'dependents',\n",
    "    'investment_horizon_years', 'employment_status_Salaried', 'sentiment_index_stationary_roll_mean_3',\n",
    "    'market_volatility_index_stationary_roll_mean_3', 'asset_ETFs', 'macroeconomic_score_stationary_roll_mean_6'\n",
    "]\n",
    "target_cols = ['forecasted_value_year_1', 'forecasted_value_year_2', 'forecasted_value_year_3']\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna(subset=selected_features + target_cols)\n",
    "\n",
    "# Normalize features\n",
    "feature_scaler = MinMaxScaler()\n",
    "df[selected_features] = feature_scaler.fit_transform(df[selected_features])\n",
    "\n",
    "# Normalize target\n",
    "target_scaler = MinMaxScaler()\n",
    "df[target_cols] = target_scaler.fit_transform(df[target_cols])\n",
    "\n",
    "# Sort by client and time step (assuming 'month' exists)\n",
    "df = df.sort_values(['client_id', 'month'])\n",
    "\n",
    "# Create sequences per client\n",
    "sequence_length = 24\n",
    "def create_sequences(data, features, targets, seq_len):\n",
    "    X, y = [], []\n",
    "    grouped = data.groupby('client_id')\n",
    "    for _, group in grouped:\n",
    "        group = group.reset_index(drop=True)\n",
    "        if len(group) >= seq_len + 1:\n",
    "            for i in range(len(group) - seq_len):\n",
    "                seq_x = group.loc[i:i+seq_len-1, features].values.astype(np.float32)\n",
    "                seq_y = group.loc[i+seq_len, targets].values.astype(np.float32)\n",
    "                X.append(seq_x)\n",
    "                y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(df, selected_features, target_cols, sequence_length)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(sequence_length, len(selected_features)), return_sequences=False),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test MAE:\", mae)\n",
    "\n",
    "# Predict and inverse transform\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_rescaled = target_scaler.inverse_transform(y_pred)\n",
    "y_test_rescaled = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "print(\"Sample predicted vs actual values:\")\n",
    "for i in range(5):\n",
    "    print(f\"Predicted: {y_pred_rescaled[i]}, Actual: {y_test_rescaled[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.01979292370378971\n",
      "Test MAE: 0.1096835657954216\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isha/Datathon/venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - loss: 0.0496 - mae: 0.1816 - val_loss: 0.0453 - val_mae: 0.1732\n",
      "Epoch 2/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0452 - mae: 0.1752 - val_loss: 0.0448 - val_mae: 0.1742\n",
      "Epoch 3/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0445 - mae: 0.1736 - val_loss: 0.0453 - val_mae: 0.1739\n",
      "Epoch 4/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0446 - mae: 0.1738 - val_loss: 0.0445 - val_mae: 0.1731\n",
      "Epoch 5/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0449 - mae: 0.1744 - val_loss: 0.0443 - val_mae: 0.1732\n",
      "Epoch 6/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0442 - mae: 0.1729 - val_loss: 0.0440 - val_mae: 0.1724\n",
      "Epoch 7/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0434 - mae: 0.1714 - val_loss: 0.0439 - val_mae: 0.1718\n",
      "Epoch 8/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0435 - mae: 0.1713 - val_loss: 0.0435 - val_mae: 0.1706\n",
      "Epoch 9/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0432 - mae: 0.1708 - val_loss: 0.0450 - val_mae: 0.1725\n",
      "Epoch 10/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0422 - mae: 0.1686 - val_loss: 0.0425 - val_mae: 0.1692\n",
      "Epoch 11/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 0.0419 - mae: 0.1678 - val_loss: 0.0433 - val_mae: 0.1698\n",
      "Epoch 12/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 0.0407 - mae: 0.1651 - val_loss: 0.0419 - val_mae: 0.1663\n",
      "Epoch 13/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 0.0408 - mae: 0.1654 - val_loss: 0.0414 - val_mae: 0.1656\n",
      "Epoch 14/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 0.0397 - mae: 0.1627 - val_loss: 0.0413 - val_mae: 0.1660\n",
      "Epoch 15/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 0.0394 - mae: 0.1618 - val_loss: 0.0398 - val_mae: 0.1617\n",
      "Epoch 16/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 0.0385 - mae: 0.1595 - val_loss: 0.0394 - val_mae: 0.1614\n",
      "Epoch 17/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 0.0371 - mae: 0.1561 - val_loss: 0.0381 - val_mae: 0.1582\n",
      "Epoch 18/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 0.0359 - mae: 0.1532 - val_loss: 0.0376 - val_mae: 0.1563\n",
      "Epoch 19/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 0.0350 - mae: 0.1509 - val_loss: 0.0377 - val_mae: 0.1569\n",
      "Epoch 20/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 0.0337 - mae: 0.1480 - val_loss: 0.0361 - val_mae: 0.1528\n",
      "Epoch 21/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - loss: 0.0331 - mae: 0.1462 - val_loss: 0.0348 - val_mae: 0.1502\n",
      "Epoch 22/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 0.0321 - mae: 0.1433 - val_loss: 0.0345 - val_mae: 0.1481\n",
      "Epoch 23/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 0.0317 - mae: 0.1426 - val_loss: 0.0342 - val_mae: 0.1491\n",
      "Epoch 24/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0308 - mae: 0.1400 - val_loss: 0.0330 - val_mae: 0.1451\n",
      "Epoch 25/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0299 - mae: 0.1376 - val_loss: 0.0324 - val_mae: 0.1438\n",
      "Epoch 26/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0294 - mae: 0.1366 - val_loss: 0.0325 - val_mae: 0.1435\n",
      "Epoch 27/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 0.0287 - mae: 0.1348 - val_loss: 0.0310 - val_mae: 0.1397\n",
      "Epoch 28/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 0.0279 - mae: 0.1325 - val_loss: 0.0312 - val_mae: 0.1411\n",
      "Epoch 29/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0274 - mae: 0.1313 - val_loss: 0.0327 - val_mae: 0.1426\n",
      "Epoch 30/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0275 - mae: 0.1315 - val_loss: 0.0317 - val_mae: 0.1412\n",
      "Epoch 31/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0268 - mae: 0.1299 - val_loss: 0.0309 - val_mae: 0.1389\n",
      "Epoch 32/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0256 - mae: 0.1267 - val_loss: 0.0292 - val_mae: 0.1350\n",
      "Epoch 33/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 0.0253 - mae: 0.1255 - val_loss: 0.0294 - val_mae: 0.1358\n",
      "Epoch 34/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0249 - mae: 0.1246 - val_loss: 0.0282 - val_mae: 0.1326\n",
      "Epoch 35/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0241 - mae: 0.1225 - val_loss: 0.0271 - val_mae: 0.1294\n",
      "Epoch 36/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - loss: 0.0236 - mae: 0.1215 - val_loss: 0.0283 - val_mae: 0.1324\n",
      "Epoch 37/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.0233 - mae: 0.1204 - val_loss: 0.0284 - val_mae: 0.1334\n",
      "Epoch 38/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.0230 - mae: 0.1194 - val_loss: 0.0273 - val_mae: 0.1306\n",
      "Epoch 39/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.0225 - mae: 0.1179 - val_loss: 0.0266 - val_mae: 0.1286\n",
      "Epoch 40/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.0220 - mae: 0.1168 - val_loss: 0.0269 - val_mae: 0.1294\n",
      "Epoch 41/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.0219 - mae: 0.1164 - val_loss: 0.0265 - val_mae: 0.1279\n",
      "Epoch 42/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.0217 - mae: 0.1159 - val_loss: 0.0244 - val_mae: 0.1229\n",
      "Epoch 43/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.0206 - mae: 0.1126 - val_loss: 0.0245 - val_mae: 0.1227\n",
      "Epoch 44/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.0204 - mae: 0.1122 - val_loss: 0.0247 - val_mae: 0.1237\n",
      "Epoch 45/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.0206 - mae: 0.1126 - val_loss: 0.0234 - val_mae: 0.1195\n",
      "Epoch 46/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.0198 - mae: 0.1102 - val_loss: 0.0244 - val_mae: 0.1223\n",
      "Epoch 47/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.0196 - mae: 0.1098 - val_loss: 0.0236 - val_mae: 0.1208\n",
      "Epoch 48/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.0193 - mae: 0.1089 - val_loss: 0.0229 - val_mae: 0.1183\n",
      "Epoch 49/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.0189 - mae: 0.1075 - val_loss: 0.0221 - val_mae: 0.1162\n",
      "Epoch 50/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.0186 - mae: 0.1066 - val_loss: 0.0232 - val_mae: 0.1191\n",
      "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0225 - mae: 0.1173\n",
      "Test Loss: 0.02210148610174656\n",
      "Test MAE: 0.1164504736661911\n",
      "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Sample predicted vs actual values:\n",
      "Predicted: [ 48385.53   87726.164 119457.47 ], Actual: [ 41681.277  80540.3   101087.8  ]\n",
      "Predicted: [ 68058.805 104188.92  130160.33 ], Actual: [ 73734.21  76344.29 125481.76]\n",
      "Predicted: [ 41838.848  91910.305 128637.56 ], Actual: [ 24251.17 106424.02 146072.3 ]\n",
      "Predicted: [ 50893.73  94277.32 133771.75], Actual: [ 63107.883 105714.55  147224.9  ]\n",
      "Predicted: [23647.6   34559.164 78076.27 ], Actual: [18272.324 39563.71  77572.63 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"/Users/isha/Datathon/engineered_stationary_data.csv\")\n",
    "\n",
    "# Define features and targets\n",
    "all_features = df.columns.tolist()\n",
    "non_stationary_cols = [\n",
    "    'monthly_contribution', 'equity_allocation_pct', 'fixed_income_allocation_pct',\n",
    "    'market_volatility_index', 'sentiment_index', 'macroeconomic_score'\n",
    "]\n",
    "target_cols = ['forecasted_value_year_1', 'forecasted_value_year_2', 'forecasted_value_year_3']\n",
    "\n",
    "# Remove target and non-stationary columns from features\n",
    "selected_features = [col for col in all_features if col not in non_stationary_cols + target_cols + ['client_id', 'month']]\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna(subset=selected_features + target_cols)\n",
    "\n",
    "# Normalize features\n",
    "feature_scaler = MinMaxScaler()\n",
    "df[selected_features] = feature_scaler.fit_transform(df[selected_features])\n",
    "\n",
    "# Normalize target\n",
    "target_scaler = MinMaxScaler()\n",
    "df[target_cols] = target_scaler.fit_transform(df[target_cols])\n",
    "\n",
    "# Sort by client and time step (assuming 'month' exists)\n",
    "df = df.sort_values(['client_id', 'month'])\n",
    "\n",
    "# Create sequences per client\n",
    "sequence_length = 24\n",
    "def create_sequences(data, features, targets, seq_len):\n",
    "    X, y = [], []\n",
    "    grouped = data.groupby('client_id')\n",
    "    for _, group in grouped:\n",
    "        group = group.reset_index(drop=True)\n",
    "        if len(group) >= seq_len + 1:\n",
    "            for i in range(len(group) - seq_len):\n",
    "                seq_x = group.loc[i:i+seq_len-1, features].values.astype(np.float32)\n",
    "                seq_y = group.loc[i+seq_len, targets].values.astype(np.float32)\n",
    "                X.append(seq_x)\n",
    "                y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(df, selected_features, target_cols, sequence_length)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(sequence_length, len(selected_features)), return_sequences=False),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test MAE:\", mae)\n",
    "\n",
    "# Predict and inverse transform\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_rescaled = target_scaler.inverse_transform(y_pred)\n",
    "y_test_rescaled = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "print(\"Sample predicted vs actual values:\")\n",
    "for i in range(5):\n",
    "    print(f\"Predicted: {y_pred_rescaled[i]}, Actual: {y_test_rescaled[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RÂ² score for forecasted_value_year_1: 0.5033\n",
      "RÂ² score for forecasted_value_year_2: 0.6203\n",
      "RÂ² score for forecasted_value_year_3: 0.5308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate RÂ² score for each forecast year\n",
    "r2_scores = {}\n",
    "for i, year in enumerate(['forecasted_value_year_1', 'forecasted_value_year_2', 'forecasted_value_year_3']):\n",
    "    r2 = r2_score(y_test_rescaled[:, i], y_pred_rescaled[:, i])\n",
    "    r2_scores[year] = r2\n",
    "    print(f\"RÂ² score for {year}: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save(\"/Users/isha/Datathon/lstm_portfolio_forecast_model.h5\")\n",
    "print(\"âœ… Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/static_time_series.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Load the dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/mnt/data/static_time_series.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(file_path)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Ensure time series columns (excluding client_id) are numeric\u001b[39;00m\n\u001b[1;32m     10\u001b[0m df\u001b[39m.\u001b[39miloc[:, \u001b[39m1\u001b[39m:\u001b[39m37\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[:, \u001b[39m1\u001b[39m:\u001b[39m37\u001b[39m]\u001b[39m.\u001b[39mapply(pd\u001b[39m.\u001b[39mto_numeric, errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoerce\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/static_time_series.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/mnt/data/static_time_series.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure time series columns (excluding client_id) are numeric\n",
    "df.iloc[:, 1:37] = df.iloc[:, 1:37].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Dictionary to store categorized clients\n",
    "client_groups = {\n",
    "    \"increasing_trend\": [],\n",
    "    \"fluctuating_trend\": [],\n",
    "    \"declining_trend\": []\n",
    "}\n",
    "\n",
    "# Function to classify clients based on trend\n",
    "def classify_client(client_series):\n",
    "    client_series = client_series[~np.isnan(client_series)]  # Remove NaN values\n",
    "    \n",
    "    if len(client_series) < 2:\n",
    "        return \"fluctuating_trend\"  # Default to fluctuating if too short\n",
    "\n",
    "    diff_series = np.diff(client_series)\n",
    "\n",
    "    if np.all(diff_series > 0):  \n",
    "        return \"increasing_trend\"\n",
    "    elif np.all(diff_series < 0):  \n",
    "        return \"declining_trend\"\n",
    "    else:\n",
    "        return \"fluctuating_trend\"\n",
    "\n",
    "# Iterate through clients and classify them\n",
    "for client in df['client_id'].unique():\n",
    "    client_data = df[df['client_id'] == client].iloc[:, 1:37].values.flatten()  # Extract time series\n",
    "    trend_type = classify_client(client_data)\n",
    "    \n",
    "    if len(client_groups[trend_type]) < 5:  # Select only 5 per category\n",
    "        client_groups[trend_type].append(client)\n",
    "\n",
    "# Function to check stationarity\n",
    "def check_stationarity(time_series):\n",
    "    time_series = time_series[~np.isnan(time_series)]  # Remove NaN values\n",
    "    \n",
    "    if len(time_series) < 2:\n",
    "        return 1.0  # Default p-value for insufficient data (not stationary)\n",
    "\n",
    "    result = adfuller(time_series)\n",
    "    return result[1]  # If p < 0.05, the series is stationary\n",
    "\n",
    "# Apply stationarity check to selected clients\n",
    "for trend, clients in client_groups.items():\n",
    "    print(f\"\\nChecking stationarity for {trend.upper()} clients:\")\n",
    "    for client in clients:\n",
    "        client_series = df[df['client_id'] == client].iloc[:, 1:37].values.flatten()\n",
    "        p_value = check_stationarity(client_series)\n",
    "        print(f\"Client {client}: p-value = {p_value:.5f} {'(Stationary)' if p_value < 0.05 else '(Non-Stationary)'}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lagged Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Feature  Lag  Correlation\n",
      "12              portfolio_value   12     0.002144\n",
      "8               portfolio_value    6     0.001779\n",
      "4               portfolio_value    3     0.001628\n",
      "0               portfolio_value    1     0.001545\n",
      "15         monthly_contribution   12     0.001063\n",
      "1         equity_allocation_pct    1     0.000813\n",
      "5         equity_allocation_pct    3     0.000764\n",
      "14  fixed_income_allocation_pct   12     0.000658\n",
      "9         equity_allocation_pct    6     0.000625\n",
      "3          monthly_contribution    1     0.000541\n",
      "7          monthly_contribution    3     0.000047\n",
      "11         monthly_contribution    6    -0.000280\n",
      "10  fixed_income_allocation_pct    6    -0.000625\n",
      "13        equity_allocation_pct   12    -0.000658\n",
      "6   fixed_income_allocation_pct    3    -0.000764\n",
      "2   fixed_income_allocation_pct    1    -0.000813\n"
     ]
    }
   ],
   "source": [
    "# Load target data\n",
    "df = pd.read_csv(\"/Users/isha/Datathon/static_time_series.csv\")\n",
    "\n",
    "# Compute lagged correlation\n",
    "correlations = []\n",
    "for lag in [1, 3, 6, 12]:\n",
    "    for feature in features:\n",
    "        df[f\"{feature}_lag_{lag}\"] = df.groupby(\"client_id\")[feature].shift(lag)\n",
    "        corr = df[[f\"{feature}_lag_{lag}\", \"forecasted_value_year_1\"]].corr().iloc[0, 1]\n",
    "        correlations.append((feature, lag, corr))\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "correlation_df = pd.DataFrame(correlations, columns=[\"Feature\", \"Lag\", \"Correlation\"])\n",
    "print(correlation_df.sort_values(by=\"Correlation\", ascending=False))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First ORder DIfferencing on Portfolio Value because of non stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Feature  Lag  Correlation\n",
      "12              portfolio_value   12     0.002144\n",
      "8               portfolio_value    6     0.001779\n",
      "4               portfolio_value    3     0.001628\n",
      "0               portfolio_value    1     0.001545\n",
      "15         monthly_contribution   12     0.001063\n",
      "1         equity_allocation_pct    1     0.000813\n",
      "5         equity_allocation_pct    3     0.000764\n",
      "14  fixed_income_allocation_pct   12     0.000658\n",
      "9         equity_allocation_pct    6     0.000625\n",
      "3          monthly_contribution    1     0.000541\n",
      "7          monthly_contribution    3     0.000047\n",
      "11         monthly_contribution    6    -0.000280\n",
      "10  fixed_income_allocation_pct    6    -0.000625\n",
      "13        equity_allocation_pct   12    -0.000658\n",
      "6   fixed_income_allocation_pct    3    -0.000764\n",
      "2   fixed_income_allocation_pct    1    -0.000813\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compute first-order differencing for portfolio_value\n",
    "df['portfolio_value_diff'] = df.groupby(\"client_id\")['portfolio_value'].diff()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "correlation_df = pd.DataFrame(correlations, columns=[\"Feature\", \"Lag\", \"Correlation\"])\n",
    "print(correlation_df.sort_values(by=\"Correlation\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>month</th>\n",
       "      <th>portfolio_value</th>\n",
       "      <th>equity_allocation_pct</th>\n",
       "      <th>fixed_income_allocation_pct</th>\n",
       "      <th>monthly_contribution</th>\n",
       "      <th>market_volatility_index</th>\n",
       "      <th>macroeconomic_score</th>\n",
       "      <th>sentiment_index</th>\n",
       "      <th>age</th>\n",
       "      <th>...</th>\n",
       "      <th>fixed_income_allocation_pct_lag_12</th>\n",
       "      <th>monthly_contribution_lag_12</th>\n",
       "      <th>portfolio_value_diff_1</th>\n",
       "      <th>portfolio_value_diff_2</th>\n",
       "      <th>portfolio_value_diff_3</th>\n",
       "      <th>portfolio_value_diff</th>\n",
       "      <th>portfolio_value_diff_lag_1</th>\n",
       "      <th>portfolio_value_diff_lag_2</th>\n",
       "      <th>portfolio_value_diff_lag_6</th>\n",
       "      <th>portfolio_value_diff_lag_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96c4c0a3-bb3f-4ac1-81ad-0850cd29911f</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>89775.68</td>\n",
       "      <td>56.07</td>\n",
       "      <td>43.93</td>\n",
       "      <td>1562.11</td>\n",
       "      <td>10.41</td>\n",
       "      <td>7.85</td>\n",
       "      <td>7.16</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89685.96</td>\n",
       "      <td>95056.55</td>\n",
       "      <td>101759.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96c4c0a3-bb3f-4ac1-81ad-0850cd29911f</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>89685.96</td>\n",
       "      <td>32.74</td>\n",
       "      <td>67.26</td>\n",
       "      <td>772.74</td>\n",
       "      <td>13.67</td>\n",
       "      <td>4.52</td>\n",
       "      <td>5.62</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90114.05</td>\n",
       "      <td>96907.97</td>\n",
       "      <td>102420.93</td>\n",
       "      <td>-89.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96c4c0a3-bb3f-4ac1-81ad-0850cd29911f</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>90114.05</td>\n",
       "      <td>56.71</td>\n",
       "      <td>43.29</td>\n",
       "      <td>709.24</td>\n",
       "      <td>15.84</td>\n",
       "      <td>4.83</td>\n",
       "      <td>5.28</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90338.07</td>\n",
       "      <td>97664.44</td>\n",
       "      <td>104344.57</td>\n",
       "      <td>428.09</td>\n",
       "      <td>-89.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96c4c0a3-bb3f-4ac1-81ad-0850cd29911f</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>90338.07</td>\n",
       "      <td>67.11</td>\n",
       "      <td>32.89</td>\n",
       "      <td>799.51</td>\n",
       "      <td>20.28</td>\n",
       "      <td>5.96</td>\n",
       "      <td>3.23</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92449.25</td>\n",
       "      <td>98037.37</td>\n",
       "      <td>106055.05</td>\n",
       "      <td>224.02</td>\n",
       "      <td>428.09</td>\n",
       "      <td>-89.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96c4c0a3-bb3f-4ac1-81ad-0850cd29911f</td>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>92449.25</td>\n",
       "      <td>23.90</td>\n",
       "      <td>76.10</td>\n",
       "      <td>1923.33</td>\n",
       "      <td>29.31</td>\n",
       "      <td>7.04</td>\n",
       "      <td>4.52</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94386.87</td>\n",
       "      <td>100066.40</td>\n",
       "      <td>106411.38</td>\n",
       "      <td>2111.18</td>\n",
       "      <td>224.02</td>\n",
       "      <td>428.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359995</th>\n",
       "      <td>41b1bc9d-b7ec-4092-803b-6ae2a1023605</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>56962.78</td>\n",
       "      <td>20.70</td>\n",
       "      <td>79.30</td>\n",
       "      <td>1862.76</td>\n",
       "      <td>20.85</td>\n",
       "      <td>3.09</td>\n",
       "      <td>5.88</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>34.23</td>\n",
       "      <td>690.39</td>\n",
       "      <td>59890.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1884.81</td>\n",
       "      <td>792.98</td>\n",
       "      <td>432.81</td>\n",
       "      <td>1529.15</td>\n",
       "      <td>910.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359996</th>\n",
       "      <td>41b1bc9d-b7ec-4092-803b-6ae2a1023605</td>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>59890.52</td>\n",
       "      <td>47.28</td>\n",
       "      <td>52.72</td>\n",
       "      <td>1926.25</td>\n",
       "      <td>22.79</td>\n",
       "      <td>4.85</td>\n",
       "      <td>4.91</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>40.89</td>\n",
       "      <td>1555.90</td>\n",
       "      <td>60708.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2927.74</td>\n",
       "      <td>1884.81</td>\n",
       "      <td>792.98</td>\n",
       "      <td>1937.97</td>\n",
       "      <td>1427.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359997</th>\n",
       "      <td>41b1bc9d-b7ec-4092-803b-6ae2a1023605</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>60708.53</td>\n",
       "      <td>31.31</td>\n",
       "      <td>68.69</td>\n",
       "      <td>1958.70</td>\n",
       "      <td>15.64</td>\n",
       "      <td>5.05</td>\n",
       "      <td>3.71</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>74.35</td>\n",
       "      <td>501.23</td>\n",
       "      <td>61984.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>818.01</td>\n",
       "      <td>2927.74</td>\n",
       "      <td>1884.81</td>\n",
       "      <td>897.60</td>\n",
       "      <td>323.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359998</th>\n",
       "      <td>41b1bc9d-b7ec-4092-803b-6ae2a1023605</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>61984.00</td>\n",
       "      <td>44.34</td>\n",
       "      <td>55.66</td>\n",
       "      <td>1284.70</td>\n",
       "      <td>26.19</td>\n",
       "      <td>3.81</td>\n",
       "      <td>5.71</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>39.77</td>\n",
       "      <td>1705.93</td>\n",
       "      <td>62743.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1275.47</td>\n",
       "      <td>818.01</td>\n",
       "      <td>2927.74</td>\n",
       "      <td>1555.02</td>\n",
       "      <td>1893.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359999</th>\n",
       "      <td>41b1bc9d-b7ec-4092-803b-6ae2a1023605</td>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>62743.82</td>\n",
       "      <td>54.93</td>\n",
       "      <td>45.07</td>\n",
       "      <td>631.58</td>\n",
       "      <td>25.81</td>\n",
       "      <td>4.51</td>\n",
       "      <td>5.41</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>34.04</td>\n",
       "      <td>820.43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>759.82</td>\n",
       "      <td>1275.47</td>\n",
       "      <td>818.01</td>\n",
       "      <td>432.81</td>\n",
       "      <td>465.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360000 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   client_id       month  portfolio_value  \\\n",
       "0       96c4c0a3-bb3f-4ac1-81ad-0850cd29911f  2022-03-01         89775.68   \n",
       "1       96c4c0a3-bb3f-4ac1-81ad-0850cd29911f  2022-04-01         89685.96   \n",
       "2       96c4c0a3-bb3f-4ac1-81ad-0850cd29911f  2022-05-01         90114.05   \n",
       "3       96c4c0a3-bb3f-4ac1-81ad-0850cd29911f  2022-06-01         90338.07   \n",
       "4       96c4c0a3-bb3f-4ac1-81ad-0850cd29911f  2022-07-01         92449.25   \n",
       "...                                      ...         ...              ...   \n",
       "359995  41b1bc9d-b7ec-4092-803b-6ae2a1023605  2024-10-01         56962.78   \n",
       "359996  41b1bc9d-b7ec-4092-803b-6ae2a1023605  2024-11-01         59890.52   \n",
       "359997  41b1bc9d-b7ec-4092-803b-6ae2a1023605  2024-12-01         60708.53   \n",
       "359998  41b1bc9d-b7ec-4092-803b-6ae2a1023605  2025-01-01         61984.00   \n",
       "359999  41b1bc9d-b7ec-4092-803b-6ae2a1023605  2025-02-01         62743.82   \n",
       "\n",
       "        equity_allocation_pct  fixed_income_allocation_pct  \\\n",
       "0                       56.07                        43.93   \n",
       "1                       32.74                        67.26   \n",
       "2                       56.71                        43.29   \n",
       "3                       67.11                        32.89   \n",
       "4                       23.90                        76.10   \n",
       "...                       ...                          ...   \n",
       "359995                  20.70                        79.30   \n",
       "359996                  47.28                        52.72   \n",
       "359997                  31.31                        68.69   \n",
       "359998                  44.34                        55.66   \n",
       "359999                  54.93                        45.07   \n",
       "\n",
       "        monthly_contribution  market_volatility_index  macroeconomic_score  \\\n",
       "0                    1562.11                    10.41                 7.85   \n",
       "1                     772.74                    13.67                 4.52   \n",
       "2                     709.24                    15.84                 4.83   \n",
       "3                     799.51                    20.28                 5.96   \n",
       "4                    1923.33                    29.31                 7.04   \n",
       "...                      ...                      ...                  ...   \n",
       "359995               1862.76                    20.85                 3.09   \n",
       "359996               1926.25                    22.79                 4.85   \n",
       "359997               1958.70                    15.64                 5.05   \n",
       "359998               1284.70                    26.19                 3.81   \n",
       "359999                631.58                    25.81                 4.51   \n",
       "\n",
       "        sentiment_index  age  ...  fixed_income_allocation_pct_lag_12  \\\n",
       "0                  7.16   63  ...                                 NaN   \n",
       "1                  5.62   63  ...                                 NaN   \n",
       "2                  5.28   63  ...                                 NaN   \n",
       "3                  3.23   63  ...                                 NaN   \n",
       "4                  4.52   63  ...                                 NaN   \n",
       "...                 ...  ...  ...                                 ...   \n",
       "359995             5.88   64  ...                               34.23   \n",
       "359996             4.91   64  ...                               40.89   \n",
       "359997             3.71   64  ...                               74.35   \n",
       "359998             5.71   64  ...                               39.77   \n",
       "359999             5.41   64  ...                               34.04   \n",
       "\n",
       "        monthly_contribution_lag_12  portfolio_value_diff_1  \\\n",
       "0                               NaN                89685.96   \n",
       "1                               NaN                90114.05   \n",
       "2                               NaN                90338.07   \n",
       "3                               NaN                92449.25   \n",
       "4                               NaN                94386.87   \n",
       "...                             ...                     ...   \n",
       "359995                       690.39                59890.52   \n",
       "359996                      1555.90                60708.53   \n",
       "359997                       501.23                61984.00   \n",
       "359998                      1705.93                62743.82   \n",
       "359999                       820.43                     NaN   \n",
       "\n",
       "        portfolio_value_diff_2  portfolio_value_diff_3  portfolio_value_diff  \\\n",
       "0                     95056.55               101759.46                   NaN   \n",
       "1                     96907.97               102420.93                -89.72   \n",
       "2                     97664.44               104344.57                428.09   \n",
       "3                     98037.37               106055.05                224.02   \n",
       "4                    100066.40               106411.38               2111.18   \n",
       "...                        ...                     ...                   ...   \n",
       "359995                     NaN                     NaN               1884.81   \n",
       "359996                     NaN                     NaN               2927.74   \n",
       "359997                     NaN                     NaN                818.01   \n",
       "359998                     NaN                     NaN               1275.47   \n",
       "359999                     NaN                     NaN                759.82   \n",
       "\n",
       "        portfolio_value_diff_lag_1  portfolio_value_diff_lag_2  \\\n",
       "0                              NaN                         NaN   \n",
       "1                              NaN                         NaN   \n",
       "2                           -89.72                         NaN   \n",
       "3                           428.09                      -89.72   \n",
       "4                           224.02                      428.09   \n",
       "...                            ...                         ...   \n",
       "359995                      792.98                      432.81   \n",
       "359996                     1884.81                      792.98   \n",
       "359997                     2927.74                     1884.81   \n",
       "359998                      818.01                     2927.74   \n",
       "359999                     1275.47                      818.01   \n",
       "\n",
       "        portfolio_value_diff_lag_6  portfolio_value_diff_lag_12  \n",
       "0                              NaN                          NaN  \n",
       "1                              NaN                          NaN  \n",
       "2                              NaN                          NaN  \n",
       "3                              NaN                          NaN  \n",
       "4                              NaN                          NaN  \n",
       "...                            ...                          ...  \n",
       "359995                     1529.15                       910.66  \n",
       "359996                     1937.97                      1427.08  \n",
       "359997                      897.60                       323.24  \n",
       "359998                     1555.02                      1893.89  \n",
       "359999                      432.81                       465.02  \n",
       "\n",
       "[360000 rows x 58 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0j/mtjzxmy9521595fqsdyysfjm0000gn/T/ipykernel_39979/382561860.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method=\"ffill\", inplace=True)\n",
      "/var/folders/0j/mtjzxmy9521595fqsdyysfjm0000gn/T/ipykernel_39979/382561860.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method=\"bfill\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Forward fill missing values\n",
    "df.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "# If any NaNs still remain at the start (e.g., first row of each client), backfill them\n",
    "df.fillna(method=\"bfill\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>month</th>\n",
       "      <th>portfolio_value</th>\n",
       "      <th>equity_allocation_pct</th>\n",
       "      <th>fixed_income_allocation_pct</th>\n",
       "      <th>monthly_contribution</th>\n",
       "      <th>market_volatility_index</th>\n",
       "      <th>macroeconomic_score</th>\n",
       "      <th>sentiment_index</th>\n",
       "      <th>age</th>\n",
       "      <th>...</th>\n",
       "      <th>fixed_income_allocation_pct_lag_12</th>\n",
       "      <th>monthly_contribution_lag_12</th>\n",
       "      <th>portfolio_value_diff</th>\n",
       "      <th>portfolio_value_diff_lag_1</th>\n",
       "      <th>portfolio_value_diff_lag_3</th>\n",
       "      <th>portfolio_value_diff_lag_6</th>\n",
       "      <th>portfolio_value_diff_lag_12</th>\n",
       "      <th>portfolio_value_diff_1</th>\n",
       "      <th>portfolio_value_diff_2</th>\n",
       "      <th>portfolio_value_diff_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96c4c0a3-bb3f-4ac1-81ad-0850cd29911f</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>89775.68</td>\n",
       "      <td>56.07</td>\n",
       "      <td>43.93</td>\n",
       "      <td>1562.11</td>\n",
       "      <td>10.41</td>\n",
       "      <td>7.85</td>\n",
       "      <td>7.16</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>43.93</td>\n",
       "      <td>1562.11</td>\n",
       "      <td>-89.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>894.95</td>\n",
       "      <td>1968.29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96c4c0a3-bb3f-4ac1-81ad-0850cd29911f</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>89685.96</td>\n",
       "      <td>32.74</td>\n",
       "      <td>67.26</td>\n",
       "      <td>772.74</td>\n",
       "      <td>13.67</td>\n",
       "      <td>4.52</td>\n",
       "      <td>5.62</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>43.93</td>\n",
       "      <td>1562.11</td>\n",
       "      <td>-89.72</td>\n",
       "      <td>-89.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>661.47</td>\n",
       "      <td>685.26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96c4c0a3-bb3f-4ac1-81ad-0850cd29911f</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>90114.05</td>\n",
       "      <td>56.71</td>\n",
       "      <td>43.29</td>\n",
       "      <td>709.24</td>\n",
       "      <td>15.84</td>\n",
       "      <td>4.83</td>\n",
       "      <td>5.28</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>43.93</td>\n",
       "      <td>1562.11</td>\n",
       "      <td>428.09</td>\n",
       "      <td>-89.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1923.64</td>\n",
       "      <td>273.14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96c4c0a3-bb3f-4ac1-81ad-0850cd29911f</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>90338.07</td>\n",
       "      <td>67.11</td>\n",
       "      <td>32.89</td>\n",
       "      <td>799.51</td>\n",
       "      <td>20.28</td>\n",
       "      <td>5.96</td>\n",
       "      <td>3.23</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>43.93</td>\n",
       "      <td>1562.11</td>\n",
       "      <td>224.02</td>\n",
       "      <td>428.09</td>\n",
       "      <td>-89.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1710.48</td>\n",
       "      <td>1144.78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96c4c0a3-bb3f-4ac1-81ad-0850cd29911f</td>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>92449.25</td>\n",
       "      <td>23.90</td>\n",
       "      <td>76.10</td>\n",
       "      <td>1923.33</td>\n",
       "      <td>29.31</td>\n",
       "      <td>7.04</td>\n",
       "      <td>4.52</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>43.93</td>\n",
       "      <td>1562.11</td>\n",
       "      <td>2111.18</td>\n",
       "      <td>224.02</td>\n",
       "      <td>-89.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>356.33</td>\n",
       "      <td>1654.98</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              client_id       month  portfolio_value  \\\n",
       "0  96c4c0a3-bb3f-4ac1-81ad-0850cd29911f  2022-03-01         89775.68   \n",
       "1  96c4c0a3-bb3f-4ac1-81ad-0850cd29911f  2022-04-01         89685.96   \n",
       "2  96c4c0a3-bb3f-4ac1-81ad-0850cd29911f  2022-05-01         90114.05   \n",
       "3  96c4c0a3-bb3f-4ac1-81ad-0850cd29911f  2022-06-01         90338.07   \n",
       "4  96c4c0a3-bb3f-4ac1-81ad-0850cd29911f  2022-07-01         92449.25   \n",
       "\n",
       "   equity_allocation_pct  fixed_income_allocation_pct  monthly_contribution  \\\n",
       "0                  56.07                        43.93               1562.11   \n",
       "1                  32.74                        67.26                772.74   \n",
       "2                  56.71                        43.29                709.24   \n",
       "3                  67.11                        32.89                799.51   \n",
       "4                  23.90                        76.10               1923.33   \n",
       "\n",
       "   market_volatility_index  macroeconomic_score  sentiment_index  age  ...  \\\n",
       "0                    10.41                 7.85             7.16   63  ...   \n",
       "1                    13.67                 4.52             5.62   63  ...   \n",
       "2                    15.84                 4.83             5.28   63  ...   \n",
       "3                    20.28                 5.96             3.23   63  ...   \n",
       "4                    29.31                 7.04             4.52   63  ...   \n",
       "\n",
       "   fixed_income_allocation_pct_lag_12  monthly_contribution_lag_12  \\\n",
       "0                               43.93                      1562.11   \n",
       "1                               43.93                      1562.11   \n",
       "2                               43.93                      1562.11   \n",
       "3                               43.93                      1562.11   \n",
       "4                               43.93                      1562.11   \n",
       "\n",
       "   portfolio_value_diff  portfolio_value_diff_lag_1  \\\n",
       "0                -89.72                         NaN   \n",
       "1                -89.72                      -89.72   \n",
       "2                428.09                      -89.72   \n",
       "3                224.02                      428.09   \n",
       "4               2111.18                      224.02   \n",
       "\n",
       "   portfolio_value_diff_lag_3  portfolio_value_diff_lag_6  \\\n",
       "0                         NaN                         NaN   \n",
       "1                         NaN                         NaN   \n",
       "2                         NaN                         NaN   \n",
       "3                      -89.72                         NaN   \n",
       "4                      -89.72                         NaN   \n",
       "\n",
       "   portfolio_value_diff_lag_12  portfolio_value_diff_1  \\\n",
       "0                          NaN                  894.95   \n",
       "1                          NaN                  661.47   \n",
       "2                          NaN                 1923.64   \n",
       "3                          NaN                 1710.48   \n",
       "4                          NaN                  356.33   \n",
       "\n",
       "   portfolio_value_diff_2  portfolio_value_diff_3  \n",
       "0                 1968.29                     NaN  \n",
       "1                  685.26                     NaN  \n",
       "2                  273.14                     NaN  \n",
       "3                 1144.78                     NaN  \n",
       "4                 1654.98                     NaN  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define the selected features and lags\n",
    "selected_lags = {\n",
    "    \"portfolio_value_diff\": [1, 3, 6, 12],  # Differenced values\n",
    "    \"equity_allocation_pct\": [1],\n",
    "    \"fixed_income_allocation_pct\": [1],\n",
    "    \"monthly_contribution\": [1],\n",
    "}\n",
    "\n",
    "# Apply lags\n",
    "for feature, lags in selected_lags.items():\n",
    "    for lag in lags:\n",
    "        df[f\"{feature}_lag_{lag}\"] = df.groupby(\"client_id\")[feature].shift(lag)\n",
    "\n",
    "# Display the first few rows to check the new features\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 16)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m# Scale features\u001b[39;00m\n\u001b[1;32m     26\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[0;32m---> 27\u001b[0m df_lstm[features] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(df_lstm[features])\n\u001b[1;32m     29\u001b[0m \u001b[39m# Split data (24 months train, 6 months validation, 6 months test)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39m24\u001b[39m\n",
      "File \u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis object (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m) has a `transform`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[39mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:930\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \n\u001b[1;32m    900\u001b[0m \u001b[39mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[39m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    929\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 930\u001b[0m X \u001b[39m=\u001b[39m validate_data(\n\u001b[1;32m    931\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    932\u001b[0m     X,\n\u001b[1;32m    933\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    934\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m    935\u001b[0m     ensure_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    936\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[1;32m    937\u001b[0m )\n\u001b[1;32m    938\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    940\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m   2945\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/sklearn/utils/validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1131\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1132\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1133\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1134\u001b[0m         )\n\u001b[1;32m   1136\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   1137\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 16)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define target variable (forecasting next 3 years)\n",
    "target = [\"portfolio_value_diff_1\", \"portfolio_value_diff_2\", \"portfolio_value_diff_3\"]\n",
    "\n",
    "# Create new target columns for the next 3 years\n",
    "df[\"portfolio_value_diff_1\"] = df.groupby(\"client_id\")[\"portfolio_value\"].shift(-12)\n",
    "df[\"portfolio_value_diff_2\"] = df.groupby(\"client_id\")[\"portfolio_value\"].shift(-24)\n",
    "df[\"portfolio_value_diff_3\"] = df.groupby(\"client_id\")[\"portfolio_value\"].shift(-36)\n",
    "\n",
    "# Select features (lagged variables)\n",
    "features = [col for col in df.columns if \"_lag_\" in col]\n",
    "\n",
    "# Handle missing values: Drop only target-related NaNs\n",
    "df_lstm = df.dropna(subset=target).copy()\n",
    "\n",
    "# **NEW FIX: Replace remaining NaNs in features with 0 or interpolation**\n",
    "df_lstm[features] = df_lstm[features].fillna(0)  # or use df_lstm[features].interpolate()\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "df_lstm[features] = scaler.fit_transform(df_lstm[features])\n",
    "\n",
    "# Split data (24 months train, 6 months validation, 6 months test)\n",
    "train_size = 24\n",
    "val_size = 6\n",
    "test_size = 6\n",
    "\n",
    "X = df_lstm[features].values\n",
    "y = df_lstm[target].values  # Now has 3 columns (year 1, 2, 3)\n",
    "\n",
    "X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:train_size+val_size+test_size]\n",
    "y_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:train_size+val_size+test_size]\n",
    "\n",
    "# Reshape input for LSTM (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(1, X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(3)  # 3 outputs for year 1, year 2, year 3\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Predict for next 3 years\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Store predictions in DataFrame\n",
    "df_lstm.loc[df_lstm.index[-len(y_pred):], [\"forecasted_value_year_1\", \"forecasted_value_year_2\", \"forecasted_value_year_3\"]] = y_pred\n",
    "\n",
    "# Save results\n",
    "df_lstm.to_csv(\"forecasted_lstm_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0j/mtjzxmy9521595fqsdyysfjm0000gn/T/ipykernel_39979/835582258.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# --------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"static_time_series.csv\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update path if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Ensure sorting by time for each client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"client_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# --------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 2. Define Features & Target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7168\u001b[0m                 \u001b[0;34mf\"Length of ascending ({len(ascending)})\"\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7169\u001b[0m                 \u001b[0;34mf\" != length of by ({len(by)})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7170\u001b[0m             )\n\u001b[1;32m   7171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7172\u001b[0;31m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7174\u001b[0m             \u001b[0;31m# need to rewrap columns in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m-> 7172\u001b[0;31m         \u001b[0;34m...\u001b[0m     \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_natsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Datathon/venv/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --------------------------------------\n",
    "# 1. Load Data\n",
    "# --------------------------------------\n",
    "df = pd.read_csv(\"static_time_series.csv\")  # Update path if needed\n",
    "\n",
    "# Ensure sorting by time for each client\n",
    "df = df.sort_values([\"client_id\", \"date\"])\n",
    "\n",
    "# --------------------------------------\n",
    "# 2. Define Features & Target\n",
    "# --------------------------------------\n",
    "features = [\"portfolio_value\", \"equity_allocation_pct\", \"fixed_income_allocation_pct\", \"monthly_contribution\"]\n",
    "target = [\"portfolio_value_year_1\", \"portfolio_value_year_2\", \"portfolio_value_year_3\"]\n",
    "\n",
    "# Create shifted targets (forecast next 3 years)\n",
    "df[\"portfolio_value_year_1\"] = df.groupby(\"client_id\")[\"portfolio_value\"].shift(-12)\n",
    "df[\"portfolio_value_year_1\"] = df.groupby(\"client_id\")[\"portfolio_value\"].shift(-24)\n",
    "df[\"portfolio_value_year_1\"] = df.groupby(\"client_id\")[\"portfolio_value\"].shift(-36)\n",
    "\n",
    "# Drop rows with NaNs (due to shifting)\n",
    "df = df.dropna(subset=target).reset_index(drop=True)\n",
    "\n",
    "# --------------------------------------\n",
    "# 3. Scale Features & Target\n",
    "# --------------------------------------\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "df[features] = scaler_X.fit_transform(df[features])\n",
    "df[target] = scaler_y.fit_transform(df[target])\n",
    "\n",
    "# --------------------------------------\n",
    "# 4. Create Sequences for LSTM\n",
    "# --------------------------------------\n",
    "time_steps = 12  # Use last 12 months to predict future\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(df) - time_steps):\n",
    "    X.append(df[features].iloc[i : i + time_steps].values)  # 12 months history\n",
    "    y.append(df[target].iloc[i + time_steps].values)  # Next 3 years\n",
    "\n",
    "X, y = np.array(X), np.array(y)  # Convert to NumPy arrays\n",
    "\n",
    "# --------------------------------------\n",
    "# 5. Train/Test Split\n",
    "# --------------------------------------\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size : train_size + val_size], y[train_size : train_size + val_size]\n",
    "X_test, y_test = X[train_size + val_size :], y[train_size + val_size :]\n",
    "\n",
    "# --------------------------------------\n",
    "# 6. Define LSTM Model\n",
    "# --------------------------------------\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(time_steps, len(features))),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(3)  # 3 outputs for 1yr, 2yr, 3yr forecasts\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 7. Train Model\n",
    "# --------------------------------------\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# --------------------------------------\n",
    "# 8. Make Predictions\n",
    "# --------------------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse Transform to get real values\n",
    "y_pred = scaler_y.inverse_transform(y_pred)\n",
    "\n",
    "# Store Predictions\n",
    "df_test = df.iloc[train_size + val_size + time_steps:].copy()\n",
    "df_test[[\"forecasted_value_year_1\", \"forecasted_value_year_1\", \"forecasted_value_year_1\"]] = y_pred\n",
    "\n",
    "# Save to CSV\n",
    "df_test.to_csv(\"forecasted_lstm_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce5c25ffc87ac977a924a9a1e43b6daaf0e6621c9dcd4a9d2e2b8e3a86fb3169"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
