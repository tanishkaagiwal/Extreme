{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"E:/Hackathon/time_series_data.csv\")\n",
    "df['month'] = pd.to_datetime(df['month'])\n",
    "df.set_index('month', inplace=True)\n",
    "print(df.info())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "df = df.interpolate(method='linear')  # Linear interpolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(12, len(df.columns) * 4))\n",
    "\n",
    "# Create individual box plots for each numerical column\n",
    "for i, column in enumerate(df.columns):\n",
    "    plt.subplot(len(df.columns), 1, i + 1)\n",
    "    sns.boxplot(x=df[column])\n",
    "    plt.title(f'Box Plot of {column}')\n",
    "    plt.xlabel(column)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh import select_features\n",
    "\n",
    "# Load tsfresh extracted features\n",
    "extracted_features = pd.read_csv(\"E:/Hackathon/extracted_features (1).csv\", index_col=0)  # or use directly if in memory\n",
    "\n",
    "# Load target data (forecasted value year 3)\n",
    "target_df = pd.read_csv(\"E:/Hackathon/target_data.csv\")\n",
    "target_series = target_df.set_index(\"client_id\")[\"forecasted_value_year_3\"]\n",
    "\n",
    "# Step 1: Impute missing values in features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "impute(extracted_features)\n",
    "\n",
    "# Step 2: Align features and target\n",
    "X = extracted_features.loc[target_series.index]\n",
    "y = target_series.loc[X.index]\n",
    "\n",
    "# Step 3: Feature selection (optional but recommended)\n",
    "from tsfresh import select_features\n",
    "X_selected = select_features(X, y)\n",
    "\n",
    "# Step 4: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train XGBoost\n",
    "model = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úÖ RMSE: {rmse:.2f}\")\n",
    "print(f\"‚úÖ R¬≤ Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh import select_features\n",
    "\n",
    "# Define targets\n",
    "targets = [\"forecasted_value_year_1\", \"forecasted_value_year_2\", \"forecasted_value_year_3\"]\n",
    "\n",
    "# Load data\n",
    "extracted_features = pd.read_csv(\"E:/Hackathon/extracted_features (1).csv\", index_col=0)\n",
    "target_df = pd.read_csv(\"E:/Hackathon/target_data.csv\")\n",
    "\n",
    "# Prepare targets\n",
    "target_df = target_df.set_index(\"client_id\")\n",
    "y_all = target_df[targets]\n",
    "\n",
    "# Align features with targets\n",
    "X = extracted_features.loc[y_all.index]\n",
    "impute(X)\n",
    "\n",
    "# Use year 3 for feature selection (proxy)\n",
    "X_selected = select_features(X, y_all[\"forecasted_value_year_3\"])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model\n",
    "base_model = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Multi-output wrapper\n",
    "multi_model = MultiOutputRegressor(base_model)\n",
    "multi_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = multi_model.predict(X_test)\n",
    "\n",
    "for i, year in enumerate(targets):\n",
    "    rmse = np.sqrt(mean_squared_error(y_test.iloc[:, i], y_pred[:, i]))\n",
    "    r2 = r2_score(y_test.iloc[:, i], y_pred[:, i])\n",
    "    print(f\"\\nüîÆ {year}\")\n",
    "    print(f\"‚úÖ RMSE: {rmse:.2f}\")\n",
    "    print(f\"‚úÖ R¬≤ Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh import select_features\n",
    "\n",
    "# Load data\n",
    "extracted_features = pd.read_csv(\"E:/Hackathon/extracted_features (1).csv\", index_col=0)\n",
    "target_df = pd.read_csv(\"E:/Hackathon/target_data.csv\")\n",
    "\n",
    "# Define targets\n",
    "targets = [\"forecasted_value_year_1\", \"forecasted_value_year_2\", \"forecasted_value_year_3\"]\n",
    "\n",
    "for year in targets:\n",
    "    print(f\"\\nüîÆ Predicting {year}...\")\n",
    "\n",
    "    y = target_df.set_index(\"client_id\")[year]\n",
    "\n",
    "    # Align\n",
    "    common_index = extracted_features.index.intersection(y.index)\n",
    "    X = extracted_features.loc[common_index]\n",
    "    y = y.loc[common_index]\n",
    "\n",
    "    # Drop missing target values\n",
    "    non_null_idx = y.dropna().index\n",
    "    X = X.loc[non_null_idx]\n",
    "    y = y.loc[non_null_idx]\n",
    "\n",
    "    # Impute missing values in features\n",
    "    impute(X)\n",
    "\n",
    "    # Feature selection\n",
    "    X_selected = select_features(X, y)\n",
    "\n",
    "    # ‚ö†Ô∏è Skip if no features were selected\n",
    "    if X_selected.shape[1] == 0:\n",
    "        print(f\"‚ö†Ô∏è No features selected for {year}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"‚úÖ RMSE: {rmse:.2f}\")\n",
    "    print(f\"‚úÖ R¬≤ Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "# Load data\n",
    "extracted_features = pd.read_csv(\"E:/Hackathon/extracted_features (1).csv\", index_col=0)\n",
    "target_df = pd.read_csv(\"E:/Hackathon/target_data.csv\")\n",
    "\n",
    "# Define target columns\n",
    "targets = [\"forecasted_value_year_1\", \"forecasted_value_year_2\", \"forecasted_value_year_3\"]\n",
    "\n",
    "# Step 1: Clean features\n",
    "# Drop columns with all NaNs or all zeros\n",
    "cleaned_features = extracted_features.dropna(axis=1)  # Drop columns with NaNs\n",
    "cleaned_features = cleaned_features.loc[:, (cleaned_features != 0).any(axis=0)]  # Drop all-zero columns\n",
    "\n",
    "# Impute missing (in case anything slipped through)\n",
    "impute(cleaned_features)\n",
    "\n",
    "# Step 2: Loop through targets\n",
    "for year in targets:\n",
    "    print(f\"\\nüîÆ Predicting {year}...\")\n",
    "\n",
    "    # Get target column and align with features\n",
    "    y = target_df.set_index(\"client_id\")[year]\n",
    "    common_index = cleaned_features.index.intersection(y.index)\n",
    "    X = cleaned_features.loc[common_index]\n",
    "    y = y.loc[common_index]\n",
    "\n",
    "    # Drop rows with missing target values\n",
    "    valid_idx = y.dropna().index\n",
    "    X = X.loc[valid_idx]\n",
    "    y = y.loc[valid_idx]\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"‚úÖ RMSE: {rmse:.2f}\")\n",
    "    print(f\"‚úÖ R¬≤ Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = (r\"E:\\Hackathon\\time_series_data.csv\")  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define scenario adjustments\n",
    "equity_impact = -0.078\n",
    "fixed_income_impact = 0.066\n",
    "macroeconomic_score_adjustment = -1.64\n",
    "sentiment_index_adjustment = 0.11\n",
    "\n",
    "# Apply adjustments\n",
    "df[\"equity_allocation_pct\"] *= (1 + equity_impact)\n",
    "df[\"fixed_income_allocation_pct\"] *= (1 + fixed_income_impact)\n",
    "df[\"macroeconomic_score\"] += macroeconomic_score_adjustment\n",
    "df[\"sentiment_index\"] += sentiment_index_adjustment\n",
    "\n",
    "# Save the modified dataset\n",
    "output_path = \"adjusted_timeseries_data.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Adjusted dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "timeseries_file = r\"E:\\Hackathon\\time_series_data.csv\"  # Replace with actual file path\n",
    "macro_file = r\"E:\\Hackathon\\macro_scenarios.csv\"  # Replace with actual file path\n",
    "\n",
    "df = pd.read_csv(timeseries_file)\n",
    "macro_df = pd.read_csv(macro_file)\n",
    "\n",
    "# Select 1000 unique clients randomly (ensuring consistency across all scenarios)\n",
    "unique_clients = df[\"client_id\"].unique()\n",
    "selected_clients = np.random.choice(unique_clients, 1000, replace=False)\n",
    "\n",
    "# Filter dataset for selected clients\n",
    "df_selected = df[df[\"client_id\"].isin(selected_clients)]\n",
    "\n",
    "# Process for each macroeconomic scenario\n",
    "for index, scenario in macro_df.iterrows():\n",
    "    scenario_id = scenario[\"scenario_id\"]\n",
    "    equity_impact = scenario[\"equity_impact\"]\n",
    "    fixed_income_impact = scenario[\"fixed_income_impact\"]\n",
    "    macroeconomic_score_adjustment = scenario[\"macroeconomic_score_adjustment\"]\n",
    "    sentiment_index_adjustment = scenario[\"sentiment_index_adjustment\"]\n",
    "    interest_rate_change = scenario[\"interest_rate_change\"]\n",
    "\n",
    "    # Create a copy of the filtered dataset to apply scenario adjustments\n",
    "    df_adjusted = df_selected.copy()\n",
    "    df_adjusted[\"equity_allocation_pct\"] *= (1 + equity_impact)\n",
    "    df_adjusted[\"fixed_income_allocation_pct\"] *= (1 + fixed_income_impact)\n",
    "    df_adjusted[\"macroeconomic_score\"] += macroeconomic_score_adjustment\n",
    "    df_adjusted[\"sentiment_index\"] += sentiment_index_adjustment\n",
    "\n",
    "    # Add new column for interest rate change\n",
    "    df_adjusted[\"interest_rate_change\"] = interest_rate_change\n",
    "\n",
    "    # Save to CSV\n",
    "    output_path = f\"adjusted_timeseries_{scenario_id}.csv\"\n",
    "    df_adjusted.to_csv(output_path, index=False)\n",
    "    print(f\"Adjusted dataset for {scenario_id} saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"E:\\Hackathon\\engineered_stationary_data.csv\")\n",
    "\n",
    "# Drop original non-stationary raw features\n",
    "non_stationary_raw_features = [\n",
    "    'portfolio_value', \n",
    "    'equity_allocation_pct', \n",
    "    'fixed_income_allocation_pct', \n",
    "    'monthly_contribution', \n",
    "    'market_volatility_index', \n",
    "    'macroeconomic_score', \n",
    "    'sentiment_index'\n",
    "]\n",
    "\n",
    "# Target columns\n",
    "target_cols = [\n",
    "    'forecasted_value_year_1',\n",
    "    'forecasted_value_year_2',\n",
    "    'forecasted_value_year_3'\n",
    "]\n",
    "\n",
    "# Clean dataframe\n",
    "df_cleaned = df.drop(columns=['client_id'] + non_stationary_raw_features)\n",
    "\n",
    "# Drop non-numeric features (like date)\n",
    "df_cleaned = df_cleaned.select_dtypes(include=[np.number])\n",
    "\n",
    "# Separate X and y\n",
    "X = df_cleaned.drop(columns=target_cols)\n",
    "y = df_cleaned[target_cols]\n",
    "\n",
    "# Random Forest Feature Selection\n",
    "important_features = set()\n",
    "\n",
    "for target in target_cols:\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y[target])\n",
    "    \n",
    "    importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "    top_features = importances.sort_values(ascending=False).head(30).index.tolist()\n",
    "    important_features.update(top_features)\n",
    "\n",
    "# Keep only selected features\n",
    "X_selected = X[list(important_features)]\n",
    "\n",
    "print(\"‚úÖ Top selected features:\")\n",
    "print(X_selected.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Define custom MSE function\n",
    "def mse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "# Load the model with custom objects\n",
    "custom_objects = {\"mse\": mse}\n",
    "model = load_model(r\"E:\\Hackathon\\lstm_portfolio_forecast_model.h5\", custom_objects=custom_objects)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"E:\\Hackathon\\adjusted_timeseries_SCN_10.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Increase market_volatility_index by 30\n",
    "df[\"market_volatility_index\"] += 30\n",
    "\n",
    "# Save the modified dataset\n",
    "output_path = \"adjusted_timeseries_SCN_10.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Modified dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"E:\\Hackathon\\time_series_data.csv\")\n",
    "df = df.drop(columns=[\"client_id\", \"month\"])\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
